{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import imagehash\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import dask\n",
    "from dask import bag, diagnostics\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "n_categories = df['category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images before preprocessing: 105392\n"
     ]
    }
   ],
   "source": [
    "print('Number of images before preprocessing:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagePath(category, file_name):\n",
    "    return f'data/train/{category:02d}/{file_name}'\n",
    "\n",
    "def displayImage(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def saveImage(fig, save_directory, p_hash):\n",
    "    fig.savefig(f'{save_directory}/{p_hash}', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 41min 28.3s\n"
     ]
    }
   ],
   "source": [
    "# remove blank images\n",
    "def checkBlankImage(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    colors = img.getcolors(img.size[0]*img.size[1])\n",
    "    img.close()\n",
    "    return len(colors) == 1\n",
    "\n",
    "list_of_dict = df.to_dict(orient='records')\n",
    "\n",
    "sequence = [getImagePath(list_of_dict[i]['category'], list_of_dict[i]['filename']) for i in range(len(list_of_dict))]\n",
    "dask_bag = bag.from_sequence(sequence).map(checkBlankImage)\n",
    "\n",
    "df = pd.DataFrame(list_of_dict)\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    df['is_blank'] = dask_bag.compute()\n",
    "\n",
    "blank_df = df[df['is_blank']==True]\n",
    "blank_df.drop(['is_blank'], axis=1, inplace=True)\n",
    "blank_df.to_csv('output/blank.csv')\n",
    "\n",
    "df = df[df['is_blank']==False]\n",
    "df.drop(['is_blank'], axis=1, inplace=True)\n",
    "df.to_csv('output/img_removed_blank.csv')\n",
    "\n",
    "# without dask: 74m40.4s\n",
    "# with dask bag: 41m29.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  7min 32.2s\n"
     ]
    }
   ],
   "source": [
    "# get p-hashes of images\n",
    "df = pd.read_csv('output/img_removed_blank.csv', index_col=0)\n",
    "\n",
    "def getImagePHash(img_path):\n",
    "    return str(imagehash.phash(Image.open(img_path)))\n",
    "\n",
    "sequence = [getImagePath(df.iloc[i]['category'], df.iloc[i]['filename']) for i in range(len(df))]\n",
    "dask_bag = bag.from_sequence(sequence).map(getImagePHash)\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    df['p-hash'] = dask_bag.compute()\n",
    "\n",
    "df.to_csv('output/img_p-hashes.csv')\n",
    "\n",
    "# group images by p-hash\n",
    "grouped_df = df.groupby('p-hash')['filename', 'category'].apply(lambda x: x.to_dict(orient='records')).reset_index(name='duplicates')\n",
    "\n",
    "# extract duplicated images\n",
    "duplicate_df = grouped_df[grouped_df['duplicates'].map(len) > 1]\n",
    "duplicate_df.to_csv('output/duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def saveDuplicatedImages(row, save_directory):\n",
    "    duplicate_list = row['duplicates']\n",
    "    n_cols = 5\n",
    "    n_rows = math.ceil(len(duplicate_list)/n_cols)\n",
    "    fig = plt.figure(figsize=([n_cols*2.5, n_rows*1.5]))\n",
    "    for n, duplicate in enumerate(eval(duplicate_list)):\n",
    "        img_path = getImagePath(duplicate['category'], duplicate['filename'])\n",
    "        image = Image.open(img_path)\n",
    "        plt.subplot(n_rows, n_cols, n+1)\n",
    "        plt.title(f\"{duplicate['category']:02d}\")\n",
    "        # plt.title(f\"{duplicate['category']:02d}\\n{duplicate['filename']}\")\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        image.close()\n",
    "    saveImage(fig, save_directory, row['p-hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  5min  4.9s\n"
     ]
    }
   ],
   "source": [
    "# save duplicated images for visualization\n",
    "duplicate_df = pd.read_csv('output/duplicates.csv', index_col=0)\n",
    "save_directory = 'output/duplicates'\n",
    "\n",
    "tasks = [saveDuplicatedImages(duplicate_df.iloc[i], save_directory) for i in range(len(duplicate_df))]\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    result = dask.compute(tasks, scheduler='processes', num_workers=8)\n",
    "\n",
    "# without dask: 29m11.7s\n",
    "# with dask: 4m48.6s (8 workers), 5m1.4s (4 workers), 8m28.1s (3 workers)\n",
    "# with dask bag: 6m4.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 38.7s\n"
     ]
    }
   ],
   "source": [
    "# group duplicated images with similar p-hashes\n",
    "duplicate_df = pd.read_csv('output/duplicates.csv', index_col=0)\n",
    "duplicate_df['duplicates'] = duplicate_df['duplicates'].apply(eval)\n",
    "# duplicate_df['similar_duplicates'] = duplicate_df.apply(lambda row: [], axis=1)\n",
    "duplicate_df['similar_p-hashes'] = duplicate_df.apply(lambda row: [], axis=1)\n",
    "\n",
    "list_of_dict = duplicate_df.to_dict(orient='records')\n",
    "\n",
    "def groupSimilarImages(item_dict):\n",
    "    current_i = item_dict['i']\n",
    "    current_item = item_dict['item']\n",
    "    for item in list_of_dict[current_i+1:]:\n",
    "        if imagehash.hex_to_hash(current_item['p-hash']) - imagehash.hex_to_hash(item['p-hash']) <= 6:\n",
    "            current_item['duplicates'] = current_item['duplicates'] + item['duplicates']\n",
    "            # current_item['similar_duplicates'] = item['duplicates']\n",
    "            current_item['similar_p-hashes'].append(item['p-hash'])\n",
    "    return current_item\n",
    "\n",
    "sequence = [{'i': i, 'item': item} for i, item in enumerate(list_of_dict)]\n",
    "dask_bag = bag.from_sequence(sequence).map(groupSimilarImages)\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    grouped_duplicates = dask_bag.compute()\n",
    "\n",
    "grouped_duplicate_df = pd.DataFrame(grouped_duplicates)\n",
    "duplicate_list = grouped_duplicate_df['similar_p-hashes'].sum()\n",
    "grouped_duplicate_df = grouped_duplicate_df[~grouped_duplicate_df['p-hash'].isin(duplicate_list)]\n",
    "\n",
    "grouped_duplicate_df.to_csv('output/duplicates_similar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  5min 53.0s\n"
     ]
    }
   ],
   "source": [
    "# save grouped similar duplicated images for visualization\n",
    "duplicate_df = pd.read_csv('output/duplicates_similar.csv', index_col=0)\n",
    "save_directory = 'output/duplicates_similar'\n",
    "\n",
    "tasks = [saveDuplicatedImages(duplicate_df.iloc[i], save_directory) for i in range(len(duplicate_df))]\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    result = dask.compute(tasks, scheduler='processes', num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check categories of duplicates\n",
    "duplicate_df = pd.read_csv('output/duplicates_similar.csv', index_col=0)\n",
    "\n",
    "def checkNumCategory(row):\n",
    "    category_list = list(set(item['category'] for item in eval(row)))\n",
    "    return len(category_list)\n",
    "\n",
    "duplicate_df['n_categories'] = duplicate_df['duplicates'].apply(checkNumCategory)\n",
    "\n",
    "duplicate_single_cat_df = duplicate_df[duplicate_df['n_categories'] == 1]\n",
    "duplicate_single_cat_df.to_csv('output/duplicates_single_category.csv')\n",
    "\n",
    "duplicate_multi_cat_df = duplicate_df[duplicate_df['n_categories'] != 1]\n",
    "duplicate_multi_cat_df.to_csv('output/duplicates_multi_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 16.4s\n"
     ]
    }
   ],
   "source": [
    "# save duplicated images which exist in multiple categories for visualization (ignore for now)\n",
    "duplicate_multi_cat_df = pd.read_csv('output/duplicates_multi_categories.csv', index_col=0)\n",
    "save_directory = 'output/duplicates_multi_categories'\n",
    "\n",
    "tasks = [saveDuplicatedImages(duplicate_multi_cat_df.iloc[i], save_directory) for i in range(len(duplicate_multi_cat_df))]\n",
    "\n",
    "with diagnostics.ProgressBar():\n",
    "    result = dask.compute(tasks, scheduler='processes', num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates that exist in the same category\n",
    "duplicate_single_cat_df = pd.read_csv('output/duplicates_single_category.csv', index_col=0)\n",
    "\n",
    "def extractDuplicatesToRemove(row):\n",
    "    return [item['filename'] for item in eval(row)[1:]]\n",
    "\n",
    "duplicates_to_remove = duplicate_single_cat_df['duplicates'].apply(extractDuplicatesToRemove).sum()\n",
    "\n",
    "df = pd.read_csv('output/img_removed_blank.csv', index_col=0)\n",
    "\n",
    "removed_duplicate_single_cat_df = df[~df['filename'].isin(duplicates_to_remove)]\n",
    "\n",
    "removed_duplicate_single_cat_df.to_csv('output/img_removed_duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: 11\tCurrent count: 1738\n",
      "Found 771 validated image filenames belonging to 1 classes.\n",
      "Category: 17\tCurrent count: 1399\n",
      "Found 1110 validated image filenames belonging to 1 classes.\n",
      "Category: 18\tCurrent count: 2014\n",
      "Found 495 validated image filenames belonging to 1 classes.\n",
      "Category: 29\tCurrent count: 1750\n",
      "Found 759 validated image filenames belonging to 1 classes.\n",
      "Category: 32\tCurrent count: 2094\n",
      "Found 415 validated image filenames belonging to 1 classes.\n",
      "Category: 33\tCurrent count: 561\n",
      "Found 1948 validated image filenames belonging to 1 classes.\n",
      "Category: 37\tCurrent count: 1632\n",
      "Found 877 validated image filenames belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# perform image augmentation to resample minority classes\n",
    "df = pd.read_csv('output/img_removed_duplicates.csv', index_col=0)\n",
    "\n",
    "categories = [11, 17, 18, 29, 32, 33, 37]\n",
    "target_count = 2509\n",
    "\n",
    "for category in categories:\n",
    "    selected_df = df[df['category']==category]\n",
    "\n",
    "    current_count = len(selected_df)\n",
    "    print(f'Category: {category}\\tCurrent count: {current_count}')\n",
    "\n",
    "    selected_df = selected_df.sample(n=target_count-current_count, replace=True)\n",
    "    selected_df['category'] = selected_df['category'].astype(str)\n",
    "\n",
    "    save_directory = f'output/generated/{category}'\n",
    "\n",
    "    if os.path.exists(save_directory):\n",
    "        shutil.rmtree(save_directory)\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "    image_datagen = ImageDataGenerator(\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        brightness_range=[0.5,1.2],\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        channel_shift_range=90,\n",
    "        fill_mode='reflect',\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False\n",
    "    )\n",
    "\n",
    "    image_generator = image_datagen.flow_from_dataframe(\n",
    "        dataframe=selected_df,\n",
    "        x_col='filename',\n",
    "        y_col='category',\n",
    "        directory=f'data/train/{category}',\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=64,\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        save_to_dir=save_directory,\n",
    "        save_format='jpg'\n",
    "    )\n",
    "\n",
    "    for _ in range(len(image_generator)):\n",
    "        image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6375 generated images added.\n"
     ]
    }
   ],
   "source": [
    "# create csv to consolidate generated images with original images\n",
    "df = pd.read_csv('output/img_removed_duplicates.csv', index_col=0)\n",
    "df['filepath'] = df.apply(lambda row: f\"data/train/{row['category']:02d}/{row['filename']}\", axis=1)\n",
    "\n",
    "count_before = len(df)\n",
    "\n",
    "for category in categories:\n",
    "    gen_directory = f'output/generated/{category}'\n",
    "    cat_df = pd.DataFrame()\n",
    "    cat_df['filename'] = pd.Series(os.listdir(gen_directory))\n",
    "    cat_df['category'] = category\n",
    "    cat_df['filepath'] = cat_df['filename'].apply(lambda file_name: f'{gen_directory}/{file_name}')\n",
    "    df = pd.concat([df, cat_df])\n",
    "\n",
    "count_after = len(df)\n",
    "\n",
    "print(f'{count_after - count_before} generated images added.')\n",
    "\n",
    "df.to_csv('output/img_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images after preprocessing: 108518\n"
     ]
    }
   ],
   "source": [
    "print('Number of images after preprocessing:', len(df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c05a5c83454c170fb9f3b30e12404e400875c108fc4a9568b134ee78204222f4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
